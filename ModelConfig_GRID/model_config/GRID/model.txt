Style_dubber_model_15_SPost_Duration(
  (variance_adaptor): VarianceAdaptor_softplus1(
    (duration_predictor): VariancePredictor_softplus1(
      (conv_layer): Sequential(
        (conv1d_1): Conv(
          (conv): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))
        )
        (relu_1): ReLU()
        (layer_norm_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout_1): Dropout(p=0.5, inplace=False)
        (conv1d_2): Conv(
          (conv): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))
        )
        (relu_2): ReLU()
        (layer_norm_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout_2): Dropout(p=0.5, inplace=False)
      )
      (conv_layer2): Sequential(
        (conv1d_1): Conv(
          (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
        )
        (relu_1): ReLU()
        (layer_norm_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout_1): Dropout(p=0.5, inplace=False)
        (conv1d_2): Conv(
          (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
        )
        (relu_2): ReLU()
        (layer_norm_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout_2): Dropout(p=0.5, inplace=False)
      )
      (conv_layer3): Sequential(
        (conv1d_1): Conv(
          (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
        )
        (relu_1): ReLU()
        (layer_norm_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout_1): Dropout(p=0.5, inplace=False)
        (conv1d_2): Conv(
          (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
        )
        (relu_2): ReLU()
        (layer_norm_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout_2): Dropout(p=0.5, inplace=False)
      )
      (conv): Conv1d(256, 1, kernel_size=(1,), stride=(1,), dilation=(5,))
      (softplus): Softplus(beta=1, threshold=20)
    )
    (length_regulator): LengthRegulator()
    (pitch_embedding): Embedding(256, 256)
    (energy_embedding): Embedding(256, 256)
  )
  (decoder_Condition): Decoder_Condition(
    (layer_stack): ModuleList(
      (0-23): 24 x FFTBlock_CBN(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=256, out_features=256, bias=True)
          (w_ks): Linear(in_features=256, out_features=256, bias=True)
          (w_vs): Linear(in_features=256, out_features=256, bias=True)
          (attention): ScaledDotProductAttention(
            (softmax): Softmax(dim=2)
          )
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (fc): Linear(in_features=256, out_features=256, bias=True)
          (dropout): Dropout(p=0.2, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Conv1d(256, 1024, kernel_size=(9,), stride=(1,), padding=(4,))
          (w_2): Conv1d(1024, 256, kernel_size=(1,), stride=(1,))
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.2, inplace=False)
        )
        (saln_0): StyleAdaptiveLayerNorm(
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
          (style): AffineLinear(
            (affine): Linear(in_features=256, out_features=512, bias=True)
          )
        )
        (saln_1): StyleAdaptiveLayerNorm(
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
          (style): AffineLinear(
            (affine): Linear(in_features=256, out_features=512, bias=True)
          )
        )
      )
    )
    (proj_life): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)
  )
  (mel_linear): Linear(in_features=256, out_features=80, bias=True)
  (postnet): PostNet(
    (convolutions): ModuleList(
      (0): Sequential(
        (0): ConvNorm(
          (conv): Conv1d(80, 256, kernel_size=(5,), stride=(1,), padding=(2,))
        )
        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1-3): 3 x Sequential(
        (0): ConvNorm(
          (conv): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))
        )
        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (4): Sequential(
        (0): ConvNorm(
          (conv): Conv1d(256, 80, kernel_size=(5,), stride=(1,), padding=(2,))
        )
        (1): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (saln_0): StyleAdaptiveLayerNorm(
      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=False)
      (style): AffineLinear(
        (affine): Linear(in_features=256, out_features=512, bias=True)
      )
    )
  )
  (lip_encoder): Lip_Encoder(
    (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (src_word_emb): Embedding(88, 512, padding_idx=0)
    (fc_out): Linear(in_features=512, out_features=256, bias=True)
    (layer_stack): ModuleList(
      (0-3): 4 x FFTBlock(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=512, out_features=512, bias=True)
          (w_ks): Linear(in_features=512, out_features=512, bias=True)
          (w_vs): Linear(in_features=512, out_features=512, bias=True)
          (attention): ScaledDotProductAttention(
            (softmax): Softmax(dim=2)
          )
          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (fc): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.2, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Conv1d(512, 1024, kernel_size=(9,), stride=(1,), padding=(4,))
          (w_2): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))
          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (attn_lip_text): StepwiseMonotonicMultiheadAttention(
    (energy): MultiheadEnergy(
      (w_qs): Linear(in_features=256, out_features=256, bias=True)
      (w_ks): Linear(in_features=256, out_features=256, bias=True)
      (w_vs): Linear(in_features=256, out_features=256, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (last_layer): Linear(in_features=256, out_features=256, bias=True)
    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
)